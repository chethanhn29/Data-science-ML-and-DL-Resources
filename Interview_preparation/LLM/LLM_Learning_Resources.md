
# Resources for Learning Generative AI

## Online Courses
- [Introduction to LangChain on Udemy](https://www.udemy.com/course/introduction-to-langchain/learn/lecture/39848376?start=0#overview)
- [LLM Bootcamp Spring 2023: Prompt Engineering](https://fullstackdeeplearning.com/llm-bootcamp/spring-2023/prompt-engineering/)
- [Deeplearning.ai short courses on LLM](https://www.deeplearning.ai/short-courses/langchain-for-llm-application-development/)
-  [Coursera Courses]
- [Awesome-LLM Github Repo](https://github.com/Hannibal046/Awesome-LLM)
- [Hands on LLM Github Repo](https://github.com/iusztinpaul/hands-on-llms)
- [LLM Course mlabonne Github](https://github.com/mlabonne/llm-course)
- [Nvidia LLM Course](https://www.nvidia.com/en-in/training/)
- [Weights and Bias LLM Courses](https://www.wandb.courses/pages/w-b-courses),[2](https://www.wandb.courses/courses/building-llm-powered-apps)
- [Weights and Biases LLM Finetuning](https://www.wandb.courses/courses/training-fine-tuning-LLMs)
- [Krish Naik]
- [Nicholas Renotte]
- [Data Bricks](
- [Free Generative AI & Large Language Models Courses by aciveloop](https://learn.activeloop.ai/)
- [COhere Course](https://docs.cohere.com/docs/the-cohere-platform)
- Hugging Face NLP Course
- [ LLM Notebooks ](https://mlabonne.github.io/blog/posts/2023-06-07-Decoding_strategies.html)
- [RAG From Scracth by Langchain](https://www.youtube.com/watch?v=wd7TZ4w1mSw&list=PLfaIDFEXuae2LXbO1_PKyVJiQ23ZztA0x)
- 1ï¸âƒ£ Building LLM Applications for Production
Itâ€™s easy to build LLMs, but very hard to make something production-ready with them. This article by Chip Huyen covers how to put LLMs into production. 
â¡ Link: https://lnkd.in/g8wt5CeG

2ï¸âƒ£ Awesome LLMOps GitHub Repo
This GitHub repo contains a curated list of the best LLMOps resources and tools for developers. 
â¡ Link: https://lnkd.in/dpbzGWFv
 
3ï¸âƒ£ LLMOps Course 
In this course, youâ€™ll go through the LLMOps pipeline of pre-processing training data for supervised instruction tuning, and adapt a supervised tuning pipeline to train and deploy a custom LLM. 
â¡ Link: https://lnkd.in/dJhagzi4

4ï¸âƒ£ Automated Testing for LLMOps Course 
In this course, you will learn how to create a continuous integration (CI) workflow to evaluate your LLM applications at every change for faster, safer, and more efficient application development.

## 
â¡ Link: https://lnkd.in/dqzpAbQn
ğ‹ğ‹ğŒğ¬ ğ‘ğ¨ğšğğ¦ğšğ©: A curated list of contents from #nlp fundamentals to understanding #llms .

ğŸ“Œ ğğ‹ğ
ğŸ“š ğ‚ğ¨ğ®ğ«ğ¬ğ: NLP specialization https://lnkd.in/eZQc9hTu
ğŸ“‘ ğ€ğ«ğ­ğ¢ğœğ¥ğ: NLP word representations basics: https://lnkd.in/eFqSJYxa
ğŸ“‘ ğ€ğ«ğ­ğ¢ğœğ¥ğ: Attention is all you need: https://lnkd.in/erP6xex8
ğŸ“¹ ğ•ğ¢ğğğ¨: Word embedding, Word2Vec: https://lnkd.in/eweB-sjU
ğŸ“¹ ğ•ğ¢ğğğ¨: Decoder only Transformers: https://lnkd.in/e-ewtNXJ

ğŸ“Œ ğ‹ğšğ«ğ ğ ğ‹ğšğ§ğ ğ®ğšğ ğ ğŒğ¨ğğğ¥ğ¬
ğŸ“š ğ‚ğ¨ğ®ğ«ğ¬ğ: Understanding Large Language Models https://lnkd.in/ehjGtA8U
ğŸ“š ğ‚ğ¨ğ®ğ«ğ¬ğ: Hands on LLM course by Maxime Labonne https://lnkd.in/e_xNSZyY
ğŸ“š ğ‚ğ¨ğ®ğ«ğ¬ğ: Hans on LLM by Paul Iusztin and Pau Labarta Bajo https://lnkd.in/eevAYGky
ğŸ“‘ ğ€ğ«ğ­ğ¢ğœğ¥ğ: LLM Patterns: https://lnkd.in/eJT3m9Ck

ğŸ“Œ ğ…ğ¨ğ®ğ§ğğšğ­ğ¢ğ¨ğ§ & ğğ«ğ-ğ­ğ«ğšğ¢ğ§ğğ ğŒğ¨ğğğ¥ğ¬
ğŸ“š ğ‚ğ¨ğ®ğ«ğ¬ğ: NLP course Hugging Face https://lnkd.in/e-RjcYGu
ğŸ“š ğ‚ğ¨ğ®ğ«ğ¬ğ: NLP & LLM Course by Cohere https://lnkd.in/eKys6E5U
ğŸ“š ğ‚ğ¨ğ®ğ«ğ¬ğ: Fine-tuning Large Language Models (short): https://lnkd.in/ehVRv-ZE
llms
ğŸ“‘ ğ€ğ«ğ­ğ¢ğœğ¥ğ: Improving Language Understanding by Generative Pre-Training https://lnkd.in/eg9FDgXq
ğŸ“‘ ğ€ğ«ğ­ğ¢ğœğ¥ğ: #gpt3 Language Models are Few-Shot Learners https://lnkd.in/e7CN3J-t

ğŸ“Œ ğ‘ğ€ğ† ğ’ğ²ğ¬ğ­ğğ¦ğ¬
ğŸ“š ğ‚ğ¨ğ®ğ«ğ¬ğ: Semantic Search : https://lnkd.in/eRak73U9
ğŸ“š ğ‚ğ¨ğ®ğ«ğ¬ğ: Building Applications with Vector Databases (short) by DeepLearning.AI https://lnkd.in/etxjqdJs
ğŸ“‘ ğ€ğ«ğ­ğ¢ğœğ¥ğ: #RAG for Knowledge-Intensive NLP Tasks https://lnkd.in/eGwmUbHs

ğŸ“Œ ğ†ğğ§ ğ€ğˆ
ğŸ“š ğ‚ğ¨ğ®ğ«ğ¬ğ: Generative Path by Google: https://lnkd.in/eyms3zrM
ğŸ“š ğ‚ğ¨ğ®ğ«ğ¬ğ: Full-stack LLM https://lnkd.in/eEvnJ3W7
ğŸ“š ğ‚ğ¨ğ®ğ«ğ¬ğ: Deploying GPT and Pre-trained Models https://lnkd.in/eXfhFHnj
ğŸ“š ğ‚ğ¨ğ®ğ«ğ¬ğ: RLHF (short): https://lnkd.in/e4bG_k6J
ğŸ“‘ ğ€ğ«ğ­ğ¢ğœğ¥ğ: Prompt Engineering basics: https://lnkd.in/eYK_fWQV


ğŸ“Œ ğ‹ğ‹ğŒğğğ¬
ğŸ“š ğ‚ğ¨ğ®ğ«ğ¬ğ: LLMOps (short) https://lnkd.in/e7nT4A_2
ğŸ“š ğ‚ğ¨ğ®ğ«ğ¬ğ Functions, Tools and Agents with LangChain (short) https://lnkd.in/eEWvf8-d
ğŸ“š ğ‚ğ¨ğ®ğ«ğ¬ğ : Automated testing LLMOPs (short) https://lnkd.in/e6ZCeytC

### [To improve RAG](https://www.linkedin.com/feed/update/urn:li:activity:7167030145327779840?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7167030145327779840%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29)

## Hugging Face NLP Course
- [Hugging Face NLP Course on Udemy](https://www.udemy.com/home/my-courses/learning/)

## Transformers summary

[GPT2 Explanation b Jalmar](https://jalammar.github.io/illustrated-gpt2/)

## [LLM Finetuning Notebooks ](https://github.com/ashishpatel26/LLM-Finetuning)

## [Langchain Handbook](https://www.pinecone.io/learn/series/langchain/)
## [Pineconde Vector Database Manulas](https://www.pinecone.io/learn/#missing-manuals)
## [Langchain Handbook](https://github.com/pinecone-io/examples/tree/master/learn/generation/langchain/handbook)
## [Retrieval Augmented Generation](https://aman.ai/primers/ai/RAG/)

An unmissable LLM playlist that encompasses the latest LLama2, Falcon finetuning, to advanced techniques such as LORA, Flash attention, Multiquery attention, etc and also delves into the intricate workings of chat-Gpt like RLHF etc.

ğŸŒŸ The Making of SRKGPT: Crafting an AI with Shahrukh Khan's Style | LLM | LLama v2 Finetuning youtu.be/gYPwx0DR7zc 
ğŸŒŸ Supercharging LLama-2: Enhancing Performance on Any Task with ChatGPT Dataset youtu.be/paGr-t1wSOQ
ğŸŒŸ Elevating Base Falcon Model with ChatGPT Dataset: A Game-Changing Approach youtu.be/lo11Iczb0Vc
ğŸŒŸ LLAMA-2 Open-Source LLM: Custom Fine-tuning Made Easy on a Single-GPU Colab Instance | PEFT | LORA youtu.be/8cc4bJtycOA
ğŸŒŸ Falcon Open-Source LLM: Custom Fine-tuning Made Easy on a Single-GPU Colab Instance | PEFT | LORA youtu.be/CxqZ5j3xlt0
ğŸŒŸ Conversational AI : Understanding the Technology behind Chat-GPT | GPT| RLHF | Few Shot Inferences
youtu.be/JKoJ5YIr2O4
ğŸŒŸ The Good, the Bad, & the Ugly : Exploring the Dual Nature of Conversational LLMs | Open-Source LLMs
youtu.be/MHfzoHC4kek
ğŸŒŸ Pushing the Boundaries of LLMs: Sparse & Flash Attention, Quantisation, Pruning, Distillation, LORA
youtu.be/mF7OM_XU2S4

ğŸ“ŒHands-On LLMs (Pau Labarta Bajo, Paul Iusztin & and Alexandru Razvant are also my 3 favourite creators on Linkedin!): An excellent course that focuses on the practical application of LLMs. Not just notebooks, but an actual product! 
Link: https://lnkd.in/gepah3Nz
ğŸ“ŒActiveloop & Towards AI LLMs Course: Covers theory about the history of LLMs and practical use cases!
Link: https://lnkd.in/gmxQ4MC8
ğŸ“ŒLLM Courses by Weights & Biases: Excellent courses for learning the basics of LLMs and specific modules on LLMOps included. 
Link: https://lnkd.in/gmxQ4MC8

## [Comprehensive list of Tutorials and courses on LangChain](https://python.langchain.com/docs/additional_resources/tutorials)


**I. Introduction to Transformer Architecture**
   - A. Language Model Evolution
   - B. Power of Transformer
      1. Learning Relevance
      2. Context Understanding
   - C. Self-Attention Significance
      - i. Visualization
      - ii. Language Encoding

**II. Transformer Architecture Overview**
   - A. Encoder-Decoder Components
   - B. Shared Characteristics
   - C. Diagram Structure
      - i. Input-Output Alignment

**III. Tokenization and Embedding**
   - A. Word to Number Conversion
   - B. High-Dimensional Vector Space
      - i. Preserving Context
      - ii. Word2Vec Roots

**IV. Positional Encoding**
   - A. Maintaining Word Order
   - B. Parallel Token Processing

**V. Self-Attention Mechanism**
   - A. Analyzing Token Relationships
      - i. Attention Weights
      - ii. Multi-Headed Self-Attention
         - a. Diverse Language Aspects
      - iii. Learning Linguistic Features

**VI. Output Generation**
   - A. Feed-Forward Network
   - B. Logits Generation
   - C. Softmax Layer
      - i. Probability Distribution
   - D. Final Token Selection

What is a ğ—©ğ—²ğ—°ğ˜ğ—¼ğ—¿ ğ——ğ—®ğ˜ğ—®ğ—¯ğ—®ğ˜€ğ—²?

With the rise of Foundational Models, Vector Databases skyrocketed in popularity. The truth is that a Vector Database is also useful outside of a Large Language Model context.

When it comes to Machine Learning, we often deal with Vector Embeddings. Vector Databases were created to perform specifically well when working with them:

â¡ï¸ Storing.
â¡ï¸ Updating.
â¡ï¸ Retrieving.

When we talk about retrieval, we refer to retrieving set of vectors that are most similar to a query in a form of a vector that is embedded in the same Latent space. This retrieval procedure is called Approximate Nearest Neighbour (ANN) search.

A query here could be in a form of an object like an image for which we would like to find similar images. Or it could be a question for which we want to retrieve relevant context that could later be transformed into an answer via a LLM.

Letâ€™s look into how one would interact with a Vector Database:

ğ—ªğ—¿ğ—¶ğ˜ğ—¶ğ—»ğ—´/ğ—¨ğ—½ğ—±ğ—®ğ˜ğ—¶ğ—»ğ—´ ğ——ğ—®ğ˜ğ—®.

1. Choose a ML model to be used to generate Vector Embeddings.
2. Embed any type of information: text, images, audio, tabular. Choice of ML model used for embedding will depend on the type of data.
3. Get a Vector representation of your data by running it through the Embedding Model.
4. Store additional metadata together with the Vector Embedding. This data would later be used to pre-filter or post-filter ANN search results.
5. Vector DB indexes Vector Embedding and metadata separately. There are multiple methods that can be used for creating vector indexes, some of them: Random Projection, Product Quantization, Locality-sensitive Hashing.
6. Vector data is stored together with indexes for Vector Embeddings and metadata connected to the Embedded objects.

ğ—¥ğ—²ğ—®ğ—±ğ—¶ğ—»ğ—´ ğ——ğ—®ğ˜ğ—®.

7. A query to be executed against a Vector Database will usually consist of two parts:

â¡ï¸ Data that will be used for ANN search. e.g. an image for which you want to find similar ones.
â¡ï¸ Metadata query to exclude Vectors that hold specific qualities known beforehand. E.g. given that you are looking for similar images of apartments - exclude apartments in a specific location.

8. You execute Metadata Query against the metadata index. It could be done before or after the ANN search procedure.
9. You embed the data into the Latent space with the same model that was used for writing the data to the Vector DB.
10. ANN search procedure is applied and a set of Vector embeddings are retrieved. Popular similarity measures for ANN search include: Cosine Similarity, Euclidean Distance, Dot Product.

Some popular Vector Databases: Qdrant, Pinecone, Weviate, Milvus, Faiss, Vespa.
![Vector Database](Vector_databse.gif)

This revised version simplifies the explanation, breaking down the content into clear sections. The introduction sets the stage, and subsequent sections provide a step-by-step understanding of the transformer architecture, from its components to the generation of output. Each subheading encapsulates a specific aspect, ensuring a concise and logical progression through the topic.

The introduction emphasizes the transformative impact of the transformer architecture on natural language processing, surpassing the limitations of earlier models like RNNs. The core strength lies in the ability to grasp word relevance and context within a sentence, not just neighboring words but all words. The concept of self-attention is pivotal, allowing the model to discern relationships between words irrespective of their positions. The attention map visualizes this, demonstrating how certain words, like "book," strongly connect with others, enhancing the model's language encoding capabilities.

The subsequent section delves into the transformer architecture, presenting a simplified diagram of the encoder and decoder components. These components work collaboratively, with both sharing similarities. Tokenization, the process of converting words into numerical representations, is essential before inputting them into the model. The embedding layer then maps tokenized words to vectors within a high-dimensional space, preserving contextual meaning. This embedding space concept has roots in previous algorithms like Word2Vec. The size of these vectors, exemplified as three-dimensional for simplicity, encodes relationships and meaning in the input sequence.

Positional encoding is introduced to maintain word order relevance during parallel processing of input tokens in the encoder or decoder. The subsequent self-attention layer analyzes relationships between tokens, employing multi-headed self-attention for diverse language aspects. Each attention head learns different linguistic features, and their combination contributes to a comprehensive understanding of language.

The attention weights, learned during training, guide the model's focus on relevant parts of the input sequence. The process is not a one-time affair; multi-headed self-attention ensures a nuanced understanding of various linguistic nuances. Following attention weights application, the output passes through a feed-forward network, producing logits proportional to the probability scores for each token in the vocabulary. A softmax layer normalizes these scores, generating a probability distribution for each word. The most likely predicted token is the one with the highest probability. The discussion hints at the versatility of final token selection, a concept to be explored in later course modules.


Certainly, let's break down the explanation into more concise sections, highlighting key subheadings and topics:

**Introduction to Transformer Architecture:**
The introduction underscores the transformative impact of the transformer architecture, surpassing RNNs. The self-attention mechanism allows the model to understand word relevance and context, a key feature demonstrated through attention maps.

**Overview of Transformer Architecture:**
Presenting a simplified diagram, the architecture is split into encoder and decoder components, working collaboratively. Tokenization converts words to numerical representations, and the embedding layer maps these tokens to high-dimensional vectors, encoding contextual meaning.

**Positional Encoding and Parallel Processing:**
To maintain word order relevance during parallel processing, positional encoding is introduced. This ensures the model doesn't lose the significance of word positions in the sentence.

**Self-Attention Mechanism:**
The self-attention layer analyzes relationships between input tokens, crucial for capturing contextual dependencies. Multi-headed self-attention further enhances the model's ability to understand various linguistic nuances.

**Attention Weights and Multi-Headed Self-Attention:**
Attention weights, learned during training, guide the model's focus on relevant parts of the input sequence. Multi-headed self-attention introduces diversity, allowing different heads to focus on distinct linguistic features.

**Feed-Forward Network and Logits:**
The output of self-attention is processed through a feed-forward network, producing logits proportional to the probability scores for each token. This output undergoes normalization through a softmax layer, resulting in a probability distribution for each word.

**Token Selection and Final Output:**
The most likely predicted token is determined by the one with the highest probability. The explanation hints at the flexibility of final token selection, a concept to be explored in later course modules.

This breakdown provides a more concise overview of each section, emphasizing the key subheadings and topics within the explanation of the transformer architecture.
Building large language models using the transformer architecture dramatically improved the performance of natural language tasks over the earlier generation of RNNs, and led to an explosion in regenerative capability. The power of the transformer architecture lies in its ability to learn the relevance and context of all of the words in a sentence. Not just as you see here, to each word next to its neighbor, but to every other word in a sentence. To apply attention weights to those relationships so that the model learns the relevance of each word to each other words no matter where they are in the input. This gives the algorithm the ability to learn who has the book, who could have the book, and if it's even relevant to the wider context of the document. These attention weights are learned during LLM training and you'll learn more about this later this week. This diagram is called an attention map and can be useful to illustrate the attention weights between each word and every other word. Here in this stylized example, you can see that the word book is strongly connected with or paying attention to the word teacher and the word student. This is called self-attention and the ability to learn a tension in this way across the whole input significantly approves the model's ability to encode language. Now that you've seen one of the key attributes of the transformer architecture, self-attention, let's cover at a high level how the model works. Here's a simplified diagram of the transformer architecture so that you can focus at a high level on where these processes are taking place. The transformer architecture is split into two distinct parts, the encoder and the decoder. These components work in conjunction with each other and they share a number of similarities. Also, note here, the diagram you see is derived from the original attention is all you need paper. Notice how the inputs to the model are at the bottom and the outputs are at the top, where possible we'll try to remain faithful to this throughout the course. Now, machine-learning models are just big statistical calculators and they work with numbers, not words. So before passing texts into the model to process, you must first tokenize the words. Simply put, this converts the words into numbers, with each number representing a position in a dictionary of all the possible words that the model can work with. You can choose from multiple tokenization methods. For example, token IDs matching two complete words, or using token IDs to represent parts of words. As you can see here. What's important is that once you've selected a tokenizer to train the model, you must use the same tokenizer when you generate text. Now that your input is represented as numbers, you can pass it to the embedding layer. This layer is a trainable vector embedding space, a high-dimensional space where each token is represented as a vector and occupies a unique location within that space. Each token ID in the vocabulary is matched to a multi-dimensional vector, and the intuition is that these vectors learn to encode the meaning and context of individual tokens in the input sequence. Embedding vector spaces have been used in natural language processing for some time, previous generation language algorithms like Word2vec use this concept. Don't worry if you're not familiar with this. You'll see examples of this throughout the course, and there are some links to additional resources in the reading exercises at the end of this week. Looking back at the sample sequence, you can see that in this simple case, each word has been matched to a token ID, and each token is mapped into a vector. In the original transformer paper, the vector size was actually 512, so much bigger than we can fit onto this image. For simplicity, if you imagine a vector size of just three, you could plot the words into a three-dimensional space and see the relationships between those words. You can see now how you can relate words that are located close to each other in the embedding space, and how you can calculate the distance between the words as an angle, which gives the model the ability to mathematically understand language. As you add the token vectors into the base of the encoder or the decoder, you also add positional encoding. The model processes each of the input tokens in parallel. So by adding the positional encoding, you preserve the information about the word order and don't lose the relevance of the position of the word in the sentence. Once you've summed the input tokens and the positional encodings, you pass the resulting vectors to the self-attention layer. Here, the model analyzes the relationships between the tokens in your input sequence. As you saw earlier, this allows the model to attend to different parts of the input sequence to better capture the contextual dependencies between the words. The self-attention weights that are learned during training and stored in these layers reflect the importance of each word in that input sequence to all other words in the sequence. But this does not happen just once, the transformer architecture actually has multi-headed self-attention. This means that multiple sets of self-attention weights or heads are learned in parallel independently of each other. The number of attention heads included in the attention layer varies from model to model, but numbers in the range of 12-100 are common. The intuition here is that each self-attention head will learn a different aspect of language. For example, one head may see the relationship between the people entities in our sentence. Whilst another head may focus on the activity of the sentence. Whilst yet another head may focus on some other properties such as if the words rhyme. It's important to note that you don't dictate ahead of time what aspects of language the attention heads will learn. The weights of each head are randomly initialized and given sufficient training data and time, each will learn different aspects of language. While some attention maps are easy to interpret, like the examples discussed here, others may not be. Now that all of the attention weights have been applied to your input data, the output is processed through a fully-connected feed-forward network. The output of this layer is a vector of logits proportional to the probability score for each and every token in the tokenizer dictionary. You can then pass these logits to a final softmax layer, where they are normalized into a probability score for each word. This output includes a probability for every single word in the vocabulary, so there's likely to be thousands of scores here. One single token will have a score higher than the rest. This is the most likely predicted token. But as you'll see later in the course, there are a number of methods that you can use to vary the final selection from this vector of probabilities.


### Scaling Laws
Scaling laws refer to the relationship between the model's performance and factors such as the number of parameters, the size of the training dataset, the compute budget, and the network architecture. They were discovered after a lot of experiments and are described in the Chinchilla paper. These laws provide insights into how to allocate resources when training these models optimally.

The main elements characterizing a language model are:

- The number of parameters (N) reflects the model's capacity to learn from data. More parameters allow the model to capture complex patterns in the data.
- The size of the training dataset (D) is measured in the number of tokens (small pieces of text ranging from a few words to a single character).
- FLOPs (floating point operations per second) measure the compute budget used for training.

### Hallucinations and Biases in LLMs
The term hallucinations refers to instances where AI systems generate outputs, such as text or images, that don't align with real-world facts or inputs. For example, ChatGPT might generate a plausible-sounding answer to an entirely incorrect factual question. 

Hallucinations in LLMs refer to instances where the model generates outputs that do not align with real-world facts or context. This can lead to the propagation of misinformation, especially in critical sectors like healthcare and education where the accuracy of information is of utmost importance. Similarly, bias in LLMs can result in outputs that favor certain perspectives over others, potentially leading to the reinforcement of harmful stereotypes and discrimination.
Consider an interaction where a user asks, "Who won the World Series in 2025?" If the LLM responds with, "The New York Yankees won the World Series in 2025," it's a clear case of hallucination. As of now (July 2023), the 2025 World Series hasn't taken place, so any claim about its outcome is a fabrication.

Bias in AI and LLMs is another significant issue. It refers to these models' inclination to favor specific outputs or decisions based on their training data. If the training data is predominantly from a specific region, the model might show a bias toward that region's language, culture, or perspectives. If the training data contains inherent biases, such as gender or racial bias, the AI system might produce skewed or discriminatory outputs.

For example, if a user asks an LLM, "Who is a nurse?" and it responds with, "She is a healthcare professional who cares for patients in a hospital,â€ it shows a gender bias. The model automatically associates nursing with women, which doesn't accurately reflect the reality where both men and women can be nurses.

Mitigating hallucinations and bias in AI systems involves refining model training, using verification techniques, and ensuring the training data is diverse and representative. Finding a balance between maximizing the model's potential and avoiding these issues remains challenging.

Interestingly, in creative domains like media and fiction writing, these "hallucinations" can be beneficial, enabling the generation of unique and innovative content.
